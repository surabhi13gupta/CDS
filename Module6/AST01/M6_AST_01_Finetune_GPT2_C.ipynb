{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surabhi13gupta/CDS/blob/main/Module6/AST01/M6_AST_01_Finetune_GPT2_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 1: Fine-tune GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* load and pre-process data from text file\n",
        "* load and use a pre-trained tokenizer\n",
        "* finetune a GPT-2 language model from Hugging Face's `transformers` library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nmP8OaXuO--"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The text data file is taken from one of the Project Gutenberg's eBooks named \"***The Buddha's Path of Virtue: A Translation of the Dhammapada*** by F. L. Woodward\", refer [here](https://www.gutenberg.org/files/35185/35185-h/35185-h.htm).\n",
        "\n",
        "To know more about Project Gutenberg's eBooks, refer [here](https://www.gutenberg.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK3ZixaWfhHD"
      },
      "source": [
        "### **GPT-2**\n",
        "\n",
        "In recent years, the OpenAI GPT-2 exhibited an impressive ability to write coherent and passionate essays that exceeded what current language models can produce. The GPT-2 wasn't a particularly novel architecture - its architecture is very similar to the **decoder-only transformer**. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset.\n",
        "\n",
        "Here, we are going to fine-tune the GPT2 model with the text of Project Gutenberg's eBook - The Buddha's Path of Virtue. We can expect that the model will be able to reply to the prompt related to the subject matter of this book after fine-tuning.\n",
        "\n",
        "To know more about GPT-2, refer [here](http://jalammar.github.io/illustrated-gpt2/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418163\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"7337014696\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/US_Airline_Tweets.csv"
      ],
      "metadata": {
        "id": "Jmg19E2p8II1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "365c8585-472f-4a56-d3ab-e21c17e0f2ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418163&recordId=5379\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M6_AST_01_Finetune_GPT2_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx pip install -U accelerate\")\n",
        "    ipython.magic(\"sx pip install -U transformers\")\n",
        "    ipython.magic(\"sx pip install torch\")\n",
        "    ipython.magic(\"%sx wget https://cdn.exec.talentsprint.com/static/cds/content/35185-0.txt\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfqnHAnMYfWF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Dataset\n",
        "!wget -qq 'https://cdn.exec.talentsprint.com/static/cds/content/35185-0.txt'\n",
        "print(\"Dataset Downloaded Successfully..\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iA3_eNIsH2de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChIKT30jKuo-"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Bn7925nhYk"
      },
      "source": [
        "The data is in a text file (.txt)\n",
        "\n",
        "Create functions to read text files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjhi8kpfYdlV"
      },
      "outputs": [],
      "source": [
        "# Functions to read different file types\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyMCdjk7bn3f"
      },
      "outputs": [],
      "source": [
        "# Read files/documents\n",
        "\n",
        "file_path = '/content/35185-0.txt'\n",
        "text_file = read_txt(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHEHx_HTc9Bf"
      },
      "outputs": [],
      "source": [
        "print(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ESwtGMaL5vJ"
      },
      "source": [
        "### Pre-processing\n",
        "\n",
        "- Remove any excess newline characters from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tYqGszOL-9f"
      },
      "outputs": [],
      "source": [
        "# Remove excess newline characters\n",
        "text_file = re.sub(r'\\n+', '\\n', text_file).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZleqLz-jdYU2"
      },
      "outputs": [],
      "source": [
        "print(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUO4loorLsA9"
      },
      "source": [
        "### Split the text into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au9dHz2IcOLa"
      },
      "outputs": [],
      "source": [
        "# Split the text into training and validation sets\n",
        "\n",
        "train_fraction = 0.8\n",
        "split_index = int(train_fraction * len(text_file))\n",
        "\n",
        "train_text = text_file[:split_index]\n",
        "val_text = text_file[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7YIqYzIawcO"
      },
      "outputs": [],
      "source": [
        "len(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABHKmK7ZcUlA"
      },
      "outputs": [],
      "source": [
        "# Save the training and validation data as text files\n",
        "\n",
        "with open(\"train.txt\", \"w\") as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "with open(\"val.txt\", \"w\") as f:\n",
        "    f.write(val_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1hlYtjjML1r"
      },
      "source": [
        "### Load pre-trained tokenizer - GP2Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LIwiw1qnGIc"
      },
      "source": [
        "The GPT2Tokenizer is based on ***Byte-Pair-Encoding***.\n",
        "\n",
        "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model.\n",
        "\n",
        "In BPE, new tokens are added until the desired vocabulary size is reached by learning ***merges***, which are rules to merge two elements of the existing vocabulary together into a new one.\n",
        "\n",
        "Below figure shows how the vocabulary updates as the BPE algorithm progresses.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Byte-pair-encoding.png\" width=450px>\n",
        "</center>\n",
        "\n",
        "To know more about Byte-Pair Encoding, refer [here](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#byte-pair-encoding-tokenization).\n",
        "\n",
        "<br>\n",
        "\n",
        "Some of the parameters required to create a GP2Tokenizer includes:\n",
        "\n",
        "- ***vocab_file (str):*** path to the vocabulary json file; maps token to integer ids\n",
        "\n",
        "- ***merges_file (str):*** path to the ***merges*** file; contains the merge rule; The merge rule file should have one merge rule per line. Every merge rule contains merge entities separated by a space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXxBS2ShDIYH"
      },
      "source": [
        "Here, we will instantiate a GPT-2 tokenizer from a predefined tokenizer using `from_pretrained()` method.\n",
        "\n",
        "It includes a parameter:\n",
        "\n",
        "- ***pretrained_model_name_or_path:*** It can be a string of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
        "\n",
        "    For example: *gpt2, gpt2-medium, gpt2-large, or gpt2-xl*\n",
        "\n",
        "    This will download the corresponding vocab, merges, and config files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qiMe9TAplyj"
      },
      "outputs": [],
      "source": [
        "# Set up the tokenizer\n",
        "checkpoint = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l2MihgfwEPh"
      },
      "outputs": [],
      "source": [
        "# Tokenize sample text using GP2Tokenizer\n",
        "sample_ids = tokenizer(\"Hello world\")\n",
        "sample_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXYqWSzLwaBA"
      },
      "outputs": [],
      "source": [
        "# Generate tokens for sample text\n",
        "sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids['input_ids'])\n",
        "sample_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHuZKceCwNKm"
      },
      "outputs": [],
      "source": [
        "# Generate original text back\n",
        "tokenizer.convert_tokens_to_string(sample_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InX4FOvgP0mi"
      },
      "source": [
        "### Tokenize text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crjMEbLfVOpq"
      },
      "outputs": [],
      "source": [
        "# Tokenize train text\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
        "\n",
        "# Tokenize validation text\n",
        "val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JstT8I-BW0Pd"
      },
      "outputs": [],
      "source": [
        "# Length of train and validation set\n",
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3AKDcZfnTi"
      },
      "outputs": [],
      "source": [
        "# Batch-size\n",
        "train_dataset[0].shape, val_dataset[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itcSXbZMib_Y"
      },
      "source": [
        "### Data Collator\n",
        "\n",
        "Data collators are objects that:\n",
        "\n",
        "- will form a batch by using a list of dataset elements as input\n",
        "- may apply some processing (like padding)\n",
        "\n",
        "One of the data collators, `DataCollatorForLanguageModeling`, can also apply some random data augmentation (like random masking) on the formed batch.\n",
        "\n",
        "<br>\n",
        "\n",
        "`DataCollatorForLanguageModeling` is a data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- ***tokenizer:*** The tokenizer used for encoding the data.\n",
        "- ***mlm*** (bool, optional, default=True): Whether or not to use masked language modeling.\n",
        "    - If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100).\n",
        "    - Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
        "- ***return_tensors*** (str): The type of Tensor to return. Allowable values are “np”, “pt” and “tf” for numpy array, pytorch tensor, and tensorflow tensor respectively.\n",
        "\n",
        "To know more about `DataCollatorForLanguageModeling` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_XWmIF3cmhU"
      },
      "outputs": [],
      "source": [
        "# Create a Data collator object\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uophCXjYq9MO"
      },
      "source": [
        "### Load pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuE_NdXuqvw0"
      },
      "source": [
        "***GPT2LMHeadModel*** is the GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "\n",
        "This model is a PyTorch `torch.nn.Module` subclass which can be used as a regular PyTorch Module.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- ***config (GPT2Config):*** Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "\n",
        "Here, we will instantiate a pretrained pytorch model from a pre-trained model configuration, using `from_pretrained()` method, that will load the weights associated with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxQWgssCqy7j"
      },
      "outputs": [],
      "source": [
        "# Set up the model\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM13pdhzJY8y"
      },
      "source": [
        "**Note: The training time for different GPT models with GPU for this dataset are as follows:**\n",
        "\n",
        "* **GPT-2 : ~20 minutes for 100 epochs**\n",
        "\n",
        "* **GPT-2 Medium:  ~1 hour for 100 epochs**\n",
        "\n",
        "* **GPT-2 Large : Run out of memory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdcpMx9QOPnU"
      },
      "source": [
        "### Fine-tune Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LqRO_Unfmk"
      },
      "source": [
        "Train a GPT-2 model using the provided training arguments. Save the resulting trained model and tokenizer to a specified output directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7pZfFvsopWT"
      },
      "source": [
        "The `Trainer` class provides an API for feature-complete training in PyTorch for most standard use cases.\n",
        "\n",
        "Before instantiating your Trainer, create a `TrainingArguments` to access all the points of customization during training.\n",
        "\n",
        "`TrainingArguments` parameters:\n",
        "\n",
        "- ***output_dir*** (str): The output directory where the model predictions and checkpoints will be written.\n",
        "- ***overwrite_output_dir*** (bool, optional, default=False): If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
        "- ***per_device_train_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for training.\n",
        "- ***per_device_eval_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\n",
        "- ***save_total_limit*** (int, optional): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
        "\n",
        "To know more about `TrainingArguments` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "To know more about `Trainer` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.Trainer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPZiEvn2cuPR"
      },
      "outputs": [],
      "source": [
        "# Set up the training arguments\n",
        "\n",
        "model_output_path = \"/content/gpt_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_output_path,\n",
        "    overwrite_output_dir = True,\n",
        "    per_device_train_batch_size = 4, # try with 2\n",
        "    per_device_eval_batch_size = 4,  #  try with 2\n",
        "    num_train_epochs = 100,\n",
        "    save_steps = 1_000,\n",
        "    save_total_limit = 2,\n",
        "    logging_dir = './logs',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX-EmL_dc_H7"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(model_output_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(model_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4izo_-go8cDP"
      },
      "source": [
        "### Test Model with user input prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0JzfOFRcUDI"
      },
      "source": [
        "##### Now, let us test the model with some prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8WnPwpHnz57"
      },
      "source": [
        "The `generate_response()` function takes a trained *model*, *tokenizer*, and a *prompt* string as input and generates a response using the GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeTKPArfgDJW"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=100):\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")      # 'pt' for returning pytorch tensor\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJJ3bzD9fsJv"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "\n",
        "my_model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
        "my_tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIpKkHVVf9Ly"
      },
      "outputs": [],
      "source": [
        "# Testing with given prompt 1\n",
        "\n",
        "prompt = \"What is teaching of Buddha?\"  # Replace with your desired prompt\n",
        "response = generate_response(my_model, my_tokenizer, prompt)\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBlBeSY7gDyO"
      },
      "outputs": [],
      "source": [
        "# Testing with given prompt 2\n",
        "prompt = \"what is dharma ?\"  # Replace with your desired prompt\n",
        "response = generate_response(my_model, my_tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVYPlTfbvS7z"
      },
      "outputs": [],
      "source": [
        "# Testing with given prompt 3\n",
        "\n",
        "prompt = \"how to live ?\"  # Replace with your desired prompt\n",
        "response = generate_response(my_model, my_tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHcQBzdJeB27"
      },
      "source": [
        "In the case of the GPT-2 tokenizer, the model uses a byte-pair encoding (BPE) algorithm, which tokenizes text into subword units. As a result, one word might be represented by multiple tokens.\n",
        "\n",
        "For example, if you set max_length to 50, the generated response will be limited to 50 tokens, which could be fewer than 50 words, depending on the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title The architecture of GPT is very similar to: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"the decoder-only transformer\" #@param [\"\", \"the encoder-only transformer\", \"the decoder-only transformer\", \"the encoder-decoder transformer\", \"none of the above\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Good enough\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd1117e-323e-4ba2-eb06-d8dd758d93ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please complete the setup first.\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}